{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from collections import defaultdict\n",
    "from Arxiv_Parser.paper_parser import parse_html\n",
    "from Arxiv_Parser.paper_storage import save_paper_data\n",
    "from LLM.llm import MultiLLM\n",
    "from Task_Conductor.prompts import RelevanceTask\n",
    "\n",
    "\n",
    "class ProcessingConfig:\n",
    "    \"\"\"é…ç½®ä¸­å¿ƒï¼ˆç®€åŒ–ç‰ˆï¼‰\"\"\"\n",
    "    DEFAULT_SAVE_DIR = \"papers\"\n",
    "    FILENAME_TEMPLATE = \"paper_{arxiv_id}.json\"\n",
    "    \n",
    "    def __init__(self, root_dir: str = \".\", custom_output: Optional[str] = None):\n",
    "        self.root_dir = Path(root_dir).expanduser().resolve()\n",
    "        self.custom_output = Path(custom_output) if custom_output else None\n",
    "        self._init_paths()\n",
    "    \n",
    "    def _init_paths(self):\n",
    "        \"\"\"åˆå§‹åŒ–è·¯å¾„ï¼ˆå»é™¤HTMLç¼“å­˜ï¼‰\"\"\"\n",
    "        self.STATE_DIR = self.root_dir / \".status\"\n",
    "        self.OUTPUT_DIR = self.root_dir / self.DEFAULT_SAVE_DIR\n",
    "        \n",
    "        self.STATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get_output_path(self, arxiv_id: str) -> Path:\n",
    "        \"\"\"è§£ææœ€ç»ˆè¾“å‡ºè·¯å¾„\"\"\"\n",
    "        if self.custom_output:\n",
    "            if self.custom_output.is_dir():\n",
    "                return self.custom_output / self.FILENAME_TEMPLATE.format(arxiv_id=arxiv_id)\n",
    "            return self.custom_output\n",
    "        return self.OUTPUT_DIR / self.FILENAME_TEMPLATE.format(arxiv_id=arxiv_id)\n",
    "\n",
    "class PaperProcessor:\n",
    "    \"\"\"å¢å¼ºå‹è®ºæ–‡å¤„ç†å™¨ï¼ˆæ— HTMLç¼“å­˜ï¼‰\"\"\"\n",
    "    \n",
    "    VERSION = \"2.1\"\n",
    "    ENCODING = 'utf-8'\n",
    "    \n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.llm = MultiLLM('deepseek-coder')\n",
    "        self._state = {\n",
    "            \"current_step\": None,\n",
    "            \"metadata\": {},\n",
    "            \"stats\": {\n",
    "                \"sections\": 0,\n",
    "                \"references\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process(self, url: str, keyword: Optional[str] = None) -> Path:\n",
    "        \"\"\"æ ¸å¿ƒå¤„ç†æµç¨‹\"\"\"\n",
    "        try:\n",
    "            # åˆå§‹åŒ–å…ƒæ•°æ®\n",
    "            arxiv_id = self._extract_arxiv_id(url)\n",
    "            output_path = self.config.get_output_path(arxiv_id)\n",
    "            \n",
    "            self._update_state({\n",
    "                \"metadata\": {\n",
    "                    \"source_url\": url,\n",
    "                    \"arxiv_id\": arxiv_id,\n",
    "                    \"keyword\": keyword,\n",
    "                    \"output_path\": str(output_path),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            # è·å–å¹¶å¤„ç†å†…å®¹\n",
    "            print(\"ğŸ”„ è·å–è®ºæ–‡å†…å®¹...\", end='', flush=True)\n",
    "            response = requests.get(url)\n",
    "            response.encoding = self.ENCODING\n",
    "            response.raise_for_status()\n",
    "            print(\"âœ…\")\n",
    "            \n",
    "            # è§£æå†…å®¹\n",
    "            print(\"ğŸ” è§£æè®ºæ–‡ç»“æ„...\", end='', flush=True)\n",
    "            paper_data = parse_html(response.text)\n",
    "            self._update_state({\n",
    "                \"stats\": {\n",
    "                    \"sections\": len(paper_data.get(\"sections\", [])),\n",
    "                    \"references\": len(paper_data.get(\"references\", []))\n",
    "                }\n",
    "            })\n",
    "            print(f\"âœ… æ‰¾åˆ° {self._state['stats']['sections']} ä¸ªç« èŠ‚\")\n",
    "            \n",
    "            # ä¿å­˜ç»“æœ\n",
    "            print(f\"ğŸ’¾ ä¿å­˜åˆ°ï¼š{output_path}...\", end='', flush=True)\n",
    "            save_paper_data(\n",
    "                paper_data, \n",
    "                str(output_path),\n",
    "                encoding=self.ENCODING\n",
    "            )\n",
    "            print(f\"âœ… ({output_path.stat().st_size / 1024:.1f} KB)\")\n",
    "            \n",
    "            # å…³é”®è¯åˆ†æ\n",
    "            if keyword:\n",
    "                print(f\"ğŸ” åˆ†æå…³é”®è¯ '{keyword}' ç›¸å…³æ€§...\")\n",
    "                score = self._analyze_relevance(paper_data[\"abstract\"], keyword)\n",
    "                print(f\"â­ ç›¸å…³æ€§è¯„åˆ†ï¼š{score}/1.0\")\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"step\": self._state.get(\"current_step\"),\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"message\": str(e),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "            print(f\"\\nâŒ å¤„ç†å¤±è´¥ï¼š{error_info['message']}\")\n",
    "            raise RuntimeError(json.dumps(error_info, ensure_ascii=False)) from e\n",
    "    \n",
    "    def _analyze_relevance(self, abstract: str, keyword: str) -> float:\n",
    "        \"\"\"æ‰§è¡Œç›¸å…³æ€§åˆ†æ\"\"\"\n",
    "        task = RelevanceTask(abstract, keyword)\n",
    "        response = self.llm.ask(task.generate_prompt())\n",
    "        return task.parse_model_output(response)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_arxiv_id(url: str) -> str:\n",
    "        \"\"\"å¢å¼ºå‹IDæå–\"\"\"\n",
    "        base_id = url.split(\"/\")[-1]\n",
    "        for substr in [\"v1\", \"html/\", \"pdf/\"]:\n",
    "            base_id = base_id.replace(substr, \"\")\n",
    "        return base_id.strip(\"/\")\n",
    "    \n",
    "    def _update_state(self, update_dict: dict):\n",
    "        \"\"\"æ›´æ–°çŠ¶æ€\"\"\"\n",
    "        self._state.update(update_dict)\n",
    "\n",
    "    def process_sections_with_buffer(self, paper_data: dict) -> list:\n",
    "        \"\"\"ä¿®æ­£åçš„ç« èŠ‚å¤„ç†æ–¹æ³•\"\"\"\n",
    "        buffer = []\n",
    "        section_counter = [0]  # ä½¿ç”¨å¯å˜å¯¹è±¡ä¿æŒè®¡æ•°çŠ¶æ€\n",
    "\n",
    "        def _recursive_processor(sections: list, parent: dict = None, depth: int = 0) -> None:\n",
    "            for idx, section in enumerate(sections, start=1):\n",
    "                # æ›´æ–°è·¯å¾„è®¡æ•°å™¨\n",
    "                if depth >= len(section_counter):\n",
    "                    section_counter.append(0)\n",
    "                section_counter[depth] = idx\n",
    "                current_path = '.'.join(map(str, section_counter[:depth+1]))\n",
    "                \n",
    "                # ä»…æ”¶é›†å½“å‰ç« èŠ‚çš„ç‹¬ç«‹æ–‡æœ¬ï¼ˆä¸å«å­ç« èŠ‚ï¼‰\n",
    "                buffer.append({\n",
    "                    \"section_path\": current_path,\n",
    "                    \"self_text\": section.get('text', ''),\n",
    "                    \"depth\": depth,\n",
    "                    \"title_chain\": _get_title_chain(section, parent),\n",
    "                    \"parent_path\": parent['section_path'] if parent else None,\n",
    "                    \"children\": []\n",
    "                })\n",
    "                \n",
    "                # é€’å½’å¤„ç†å­ç« èŠ‚\n",
    "                if section.get('subsections'):\n",
    "                    _recursive_processor(\n",
    "                        section['subsections'],\n",
    "                        parent=section,\n",
    "                        depth=depth + 1\n",
    "                    )\n",
    "                \n",
    "                # é‡ç½®æ·±å±‚è®¡æ•°å™¨\n",
    "                for i in range(depth+1, len(section_counter)):\n",
    "                    section_counter[i] = 0\n",
    "\n",
    "        def _get_title_chain(section: dict, parent: dict) -> str:\n",
    "            \"\"\"ç”Ÿæˆæ ‡é¢˜å±‚çº§é“¾\"\"\"\n",
    "            chain = []\n",
    "            if parent and 'title_chain' in parent:\n",
    "                chain.append(parent['title_chain'])\n",
    "            chain.append(section.get('title', ''))\n",
    "            return ' > '.join(chain)\n",
    "\n",
    "        _recursive_processor(paper_data.get('sections', []))\n",
    "        return self._build_hierarchy(buffer)\n",
    "\n",
    "    def _build_hierarchy(self, flat_list: list) -> list:\n",
    "        \"\"\"å°†æ‰å¹³åˆ—è¡¨è½¬æ¢ä¸ºæ ‘å½¢ç»“æ„\"\"\"\n",
    "        node_map = {item['section_path']: item for item in flat_list}\n",
    "        for item in flat_list:\n",
    "            if item['parent_path']:\n",
    "                parent = node_map[item['parent_path']]\n",
    "                parent['children'].append(item)\n",
    "        return [item for item in flat_list if item['parent_path'] is None]\n",
    "\n",
    "    def generate_char_stats(self, buffer: list) -> dict:\n",
    "        \"\"\"ä¿®æ­£åçš„ç»Ÿè®¡æ–¹æ³•\"\"\"\n",
    "        stats = {\n",
    "            \"individual_stats\": [],\n",
    "            \"cumulative_stats\": defaultdict(int),\n",
    "            \"summary\": defaultdict(int)\n",
    "        }\n",
    "\n",
    "        # é¢„è®¡ç®—ç‹¬ç«‹å­—ç¬¦æ•°\n",
    "        for item in buffer:\n",
    "            self_chars = len(item['self_text'])\n",
    "            stats[\"individual_stats\"].append({\n",
    "                \"path\": item[\"section_path\"],\n",
    "                \"title_chain\": item[\"title_chain\"],\n",
    "                \"self_chars\": self_chars,\n",
    "                \"cumulative_chars\": self_chars + sum(len(c['self_text']) for c in item['children'])\n",
    "            })\n",
    "            stats[\"cumulative_stats\"][item[\"section_path\"]] = self_chars\n",
    "\n",
    "        # ååºéå†è®¡ç®—ç´¯è®¡å€¼\n",
    "        def _post_order(node):\n",
    "            total = len(node['self_text'])\n",
    "            for child in node['children']:\n",
    "                total += _post_order(child)\n",
    "            stats[\"cumulative_stats\"][node['section_path']] = total\n",
    "            return total\n",
    "\n",
    "        for root in [item for item in buffer if not item['parent_path']]:\n",
    "            _post_order(root)\n",
    "\n",
    "        # ç”Ÿæˆæ‘˜è¦\n",
    "        total_self = sum(len(item['self_text']) for item in buffer)\n",
    "        stats[\"summary\"] = {\n",
    "            \"total_sections\": len(buffer),\n",
    "            \"total_self_chars\": total_self,\n",
    "            \"total_cumulative_chars\": stats[\"cumulative_stats\"].get('1', 0),\n",
    "            \"avg_self_per_section\": total_self // len(buffer) if buffer else 0,\n",
    "            \"max_depth\": max(item['depth'] for item in buffer),\n",
    "            \"longest_self_section\": max(buffer, key=lambda x: len(x['self_text']))['section_path'] if buffer else None,\n",
    "            \"longest_cumulative_section\": max(stats[\"cumulative_stats\"].items(), key=lambda x: x[1])[0]\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ProcessingConfig(\n",
    "    root_dir=r\"C:\\Users\\Inuyasha\\Programs\\Python\\AIGC\\Arxiv_Reviewer/research_papers\"\n",
    ")\n",
    "\n",
    "processor = PaperProcessor(config)\n",
    "result_path = processor.process(\n",
    "    url=\"https://arxiv.org/html/2501.00092v1\",\n",
    "    keyword=\"AI\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åœ¨paper_storage.pyä¸­å¢å¼ºä¿å­˜å‡½æ•°\n",
    "def save_paper_data(data: dict, filename: str, encoding='utf-8'):\n",
    "    \"\"\"å¢å¼ºä¿å­˜å‡½æ•°\"\"\"\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filename, 'w', encoding=encoding, errors='replace') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–é…ç½®å’Œå¤„ç†å™¨ï¼ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„è·¯å¾„ï¼‰\n",
    "config = ProcessingConfig(\n",
    "    root_dir=r\"C:\\Users\\Inuyasha\\Programs\\Python\\AIGC\\Arxiv_Reviewer/research_papers\"\n",
    ")\n",
    "processor = PaperProcessor(config)\n",
    "\n",
    "# åŠ è½½å·²å¤„ç†çš„è®ºæ–‡æ•°æ®\n",
    "paper_path = config.get_output_path(\"2501.00092\")  # æ ¹æ®å®é™…arxiv_idä¿®æ”¹\n",
    "with open(paper_path, 'r', encoding='utf-8') as f:\n",
    "    paper_data = json.load(f)\n",
    "\n",
    "# æ‰§è¡Œç¼“å†²å¤„ç†\n",
    "buffer = processor.process_sections_with_buffer(paper_data)\n",
    "\n",
    "# æŸ¥çœ‹å¤„ç†ç»“æœ\n",
    "for item in buffer[:2]:  # æŸ¥çœ‹å‰ä¸¤é¡¹ç¤ºä¾‹\n",
    "    print(f\"è·¯å¾„ï¼š{item['section_path']}\")\n",
    "    print(f\"æ ‡é¢˜é“¾ï¼š{item['title_chain']}\")\n",
    "    print(f\"æ–‡æœ¬é•¿åº¦ï¼š{len(item['full_text'])}å­—ç¬¦\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š\n",
    "stats = processor.generate_char_stats(buffer)\n",
    "\n",
    "# æ§åˆ¶å°è¾“å‡º\n",
    "processor.print_stat_report(stats)\n",
    "\n",
    "# è·å–åŸå§‹æ•°æ®ï¼ˆå¦‚éœ€ç¼–ç¨‹å¤„ç†ï¼‰\n",
    "print(\"\\nç‹¬ç«‹ç« èŠ‚ç»Ÿè®¡ç¤ºä¾‹:\")\n",
    "for item in stats[\"individual_stats\"][:2]:\n",
    "    print(f\"è·¯å¾„ {item['path']} | æ ‡é¢˜é“¾: {item['title_chain'][:20]}... | å­—ç¬¦æ•°: {item['chars']:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "åˆ†å‰²åæ¯ä¸ªå…ƒç´ çš„é•¿åº¦:\n",
      "123\n",
      "7551\n",
      "7597\n",
      "6051\n",
      "2543\n",
      "7969\n",
      "7178\n",
      "7526\n",
      "7958\n",
      "3748\n",
      "7389\n",
      "7702\n",
      "7582\n",
      "814\n",
      "3009\n",
      "2936\n",
      "5114\n",
      "7539\n",
      "5148\n",
      "7758\n",
      "7469\n",
      "6276\n",
      "7334\n",
      "8005\n",
      "7528\n",
      "6816\n",
      "7539\n",
      "7191\n",
      "8010\n",
      "5851\n",
      "7661\n",
      "8074\n",
      "7341\n",
      "7780\n",
      "7647\n",
      "7930\n",
      "7250\n",
      "7372\n",
      "7899\n",
      "2645\n",
      "7162\n",
      "7862\n",
      "7976\n",
      "5336\n",
      "2009\n",
      "6391\n",
      "7821\n",
      "3739\n",
      "7823\n",
      "7410\n",
      "1589\n",
      "7703\n",
      "6813\n",
      "7250\n",
      "5595\n",
      "7851\n",
      "1166\n",
      "7718\n",
      "7665\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "# åŠ è½½å·²å¤„ç†çš„è®ºæ–‡æ•°æ®\n",
    "paper_path = config.get_output_path(\"2501.00092\")  # æ ¹æ®å®é™…arxiv_idä¿®æ”¹\n",
    "with open(paper_path, 'r', encoding='utf-8') as f:\n",
    "    paper_data = json.load(f)\n",
    "\n",
    "def extract_content_summary(paper_data: dict) -> List[str]:\n",
    "    \"\"\"å®Œæ•´å­—æ•°ç»Ÿè®¡ä¸å†…å®¹æ‘˜è¦\"\"\"\n",
    "    result = []\n",
    "    total_chars = 0\n",
    "    section_counter = [0]\n",
    "\n",
    "    # å¤„ç†Abstract\n",
    "    if 'abstract' in paper_data and paper_data['abstract']:\n",
    "        abstract = paper_data['abstract'].strip()\n",
    "        count = len(abstract)\n",
    "        preview = abstract[:100] + ('...' if len(abstract) > 100 else '')\n",
    "        result.append(f\"[Abstract] ({count}å­—)\\n{preview}\\n\")\n",
    "        total_chars += count\n",
    "\n",
    "    # é€’å½’å¤„ç†ç« èŠ‚\n",
    "    def process_sections(sections: list, depth=0):\n",
    "        nonlocal total_chars\n",
    "        if depth >= len(section_counter):\n",
    "            section_counter.append(0)\n",
    "\n",
    "        for idx, section in enumerate(sections, start=1):\n",
    "            section_counter[depth] = idx\n",
    "            current_label = \".\".join(map(str, section_counter[:depth + 1]))\n",
    "            \n",
    "            if 'text' in section and section['text']:\n",
    "                text = section['text'].strip()\n",
    "                count = len(text)\n",
    "                preview = text\n",
    "                result.append(f\"[{current_label}] ({count}å­—)\\n{preview}\\n\")\n",
    "                total_chars += count\n",
    "\n",
    "            if 'subsections' in section and section['subsections']:\n",
    "                process_sections(section['subsections'], depth + 1)\n",
    "            \n",
    "            # é‡ç½®å­çº§ç¼–å·\n",
    "            for i in range(depth + 1, len(section_counter)):\n",
    "                section_counter[i] = 0\n",
    "\n",
    "    if 'sections' in paper_data and paper_data['sections']:\n",
    "        process_sections(paper_data['sections'])\n",
    "\n",
    "    # æ·»åŠ æ€»ç»Ÿè®¡\n",
    "    result.append(\"=\" * 16 + f\"\\næ€»å­—æ•°: {total_chars:,}å­—\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def split_long_paragraphs(content_list: List[str], max_length: int = 8000, overlap: int = 500) -> List[Dict]:\n",
    "    \"\"\"å°†é•¿æ®µè½åˆ†å‰²æˆæ›´å°çš„éƒ¨åˆ†\"\"\"\n",
    "    def split_text(text: str) -> List[str]:\n",
    "        # é¦–å…ˆæŒ‰æ¢è¡Œç¬¦åˆ†å‰²\n",
    "        lines = text.split('\\n')\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for line in lines:\n",
    "            if current_length + len(line) > max_length:\n",
    "                if current_chunk:\n",
    "                    chunks.append('\\n'.join(current_chunk))\n",
    "                    current_chunk = [current_chunk[-1]]\n",
    "                    current_length = len(current_chunk[-1])\n",
    "                \n",
    "                if len(line) > max_length:\n",
    "                    sentences = re.split('([ã€‚ï¼ï¼Ÿ])', line)\n",
    "                    temp_chunk = []\n",
    "                    for i in range(0, len(sentences), 2):\n",
    "                        sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')\n",
    "                        if current_length + len(sentence) <= max_length:\n",
    "                            temp_chunk.append(sentence)\n",
    "                            current_length += len(sentence)\n",
    "                        else:\n",
    "                            if temp_chunk:\n",
    "                                chunks.append(''.join(temp_chunk))\n",
    "                            temp_chunk = [sentence]\n",
    "                            current_length = len(sentence)\n",
    "                    current_chunk.extend(temp_chunk)\n",
    "                else:\n",
    "                    current_chunk.append(line)\n",
    "                    current_length += len(line)\n",
    "            else:\n",
    "                current_chunk.append(line)\n",
    "                current_length += len(line)\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    result = []\n",
    "    for item in content_list:\n",
    "        if len(item) <= max_length:\n",
    "            result.append({\"content\": item, \"word_count\": len(item)})\n",
    "        else:\n",
    "            split_parts = split_text(item)\n",
    "            for idx, part in enumerate(split_parts):\n",
    "                result.append({\n",
    "                    \"content\": part,\n",
    "                    \"word_count\": len(part),\n",
    "                    \"part\": idx + 1,\n",
    "                    \"total_parts\": len(split_parts)\n",
    "                })\n",
    "\n",
    "    return result\n",
    "# è·å–å†…å®¹æ‘˜è¦\n",
    "content_summary = extract_content_summary(paper_data)\n",
    "\n",
    "# æ‰“å°åˆ†å‰²å‰æ¯ä¸ªå…ƒç´ çš„é•¿åº¦\n",
    "# print(\"åˆ†å‰²å‰æ¯ä¸ªå…ƒç´ çš„é•¿åº¦:\")\n",
    "# for item in content_summary:\n",
    "#     print(len(item),item)\n",
    "\n",
    "# åˆ†å‰²é•¿æ®µè½\n",
    "split_result = split_long_paragraphs(content_summary)\n",
    "print(len(split_result))\n",
    "# æ‰“å°åˆ†å‰²åæ¯ä¸ªå…ƒç´ çš„é•¿åº¦\n",
    "print(\"åˆ†å‰²åæ¯ä¸ªå…ƒç´ çš„é•¿åº¦:\")\n",
    "for item in split_result:\n",
    "    print(len(item['content']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
