{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from collections import defaultdict\n",
    "from Arxiv_Parser.paper_parser import parse_html\n",
    "from Arxiv_Parser.paper_storage import save_paper_data\n",
    "from LLM.llm import MultiLLM\n",
    "from Task_Conductor.prompts import RelevanceTask\n",
    "\n",
    "\n",
    "class ProcessingConfig:\n",
    "    \"\"\"配置中心（简化版）\"\"\"\n",
    "    DEFAULT_SAVE_DIR = \"papers\"\n",
    "    FILENAME_TEMPLATE = \"paper_{arxiv_id}.json\"\n",
    "    \n",
    "    def __init__(self, root_dir: str = \".\", custom_output: Optional[str] = None):\n",
    "        self.root_dir = Path(root_dir).expanduser().resolve()\n",
    "        self.custom_output = Path(custom_output) if custom_output else None\n",
    "        self._init_paths()\n",
    "    \n",
    "    def _init_paths(self):\n",
    "        \"\"\"初始化路径（去除HTML缓存）\"\"\"\n",
    "        self.STATE_DIR = self.root_dir / \".status\"\n",
    "        self.OUTPUT_DIR = self.root_dir / self.DEFAULT_SAVE_DIR\n",
    "        \n",
    "        self.STATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get_output_path(self, arxiv_id: str) -> Path:\n",
    "        \"\"\"解析最终输出路径\"\"\"\n",
    "        if self.custom_output:\n",
    "            if self.custom_output.is_dir():\n",
    "                return self.custom_output / self.FILENAME_TEMPLATE.format(arxiv_id=arxiv_id)\n",
    "            return self.custom_output\n",
    "        return self.OUTPUT_DIR / self.FILENAME_TEMPLATE.format(arxiv_id=arxiv_id)\n",
    "\n",
    "class PaperProcessor:\n",
    "    \"\"\"增强型论文处理器（无HTML缓存）\"\"\"\n",
    "    \n",
    "    VERSION = \"2.1\"\n",
    "    ENCODING = 'utf-8'\n",
    "    \n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.llm = MultiLLM('deepseek-coder')\n",
    "        self._state = {\n",
    "            \"current_step\": None,\n",
    "            \"metadata\": {},\n",
    "            \"stats\": {\n",
    "                \"sections\": 0,\n",
    "                \"references\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process(self, url: str, keyword: Optional[str] = None) -> Path:\n",
    "        \"\"\"核心处理流程\"\"\"\n",
    "        try:\n",
    "            # 初始化元数据\n",
    "            arxiv_id = self._extract_arxiv_id(url)\n",
    "            output_path = self.config.get_output_path(arxiv_id)\n",
    "            \n",
    "            self._update_state({\n",
    "                \"metadata\": {\n",
    "                    \"source_url\": url,\n",
    "                    \"arxiv_id\": arxiv_id,\n",
    "                    \"keyword\": keyword,\n",
    "                    \"output_path\": str(output_path),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            # 获取并处理内容\n",
    "            print(\"🔄 获取论文内容...\", end='', flush=True)\n",
    "            response = requests.get(url)\n",
    "            response.encoding = self.ENCODING\n",
    "            response.raise_for_status()\n",
    "            print(\"✅\")\n",
    "            \n",
    "            # 解析内容\n",
    "            print(\"🔍 解析论文结构...\", end='', flush=True)\n",
    "            paper_data = parse_html(response.text)\n",
    "            self._update_state({\n",
    "                \"stats\": {\n",
    "                    \"sections\": len(paper_data.get(\"sections\", [])),\n",
    "                    \"references\": len(paper_data.get(\"references\", []))\n",
    "                }\n",
    "            })\n",
    "            print(f\"✅ 找到 {self._state['stats']['sections']} 个章节\")\n",
    "            \n",
    "            # 保存结果\n",
    "            print(f\"💾 保存到：{output_path}...\", end='', flush=True)\n",
    "            save_paper_data(\n",
    "                paper_data, \n",
    "                str(output_path),\n",
    "                encoding=self.ENCODING\n",
    "            )\n",
    "            print(f\"✅ ({output_path.stat().st_size / 1024:.1f} KB)\")\n",
    "            \n",
    "            # 关键词分析\n",
    "            if keyword:\n",
    "                print(f\"🔎 分析关键词 '{keyword}' 相关性...\")\n",
    "                score = self._analyze_relevance(paper_data[\"abstract\"], keyword)\n",
    "                print(f\"⭐ 相关性评分：{score}/1.0\")\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"step\": self._state.get(\"current_step\"),\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"message\": str(e),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "            print(f\"\\n❌ 处理失败：{error_info['message']}\")\n",
    "            raise RuntimeError(json.dumps(error_info, ensure_ascii=False)) from e\n",
    "    \n",
    "    def _analyze_relevance(self, abstract: str, keyword: str) -> float:\n",
    "        \"\"\"执行相关性分析\"\"\"\n",
    "        task = RelevanceTask(abstract, keyword)\n",
    "        response = self.llm.ask(task.generate_prompt())\n",
    "        return task.parse_model_output(response)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_arxiv_id(url: str) -> str:\n",
    "        \"\"\"增强型ID提取\"\"\"\n",
    "        base_id = url.split(\"/\")[-1]\n",
    "        for substr in [\"v1\", \"html/\", \"pdf/\"]:\n",
    "            base_id = base_id.replace(substr, \"\")\n",
    "        return base_id.strip(\"/\")\n",
    "    \n",
    "    def _update_state(self, update_dict: dict):\n",
    "        \"\"\"更新状态\"\"\"\n",
    "        self._state.update(update_dict)\n",
    "\n",
    "    def process_sections_with_buffer(self, paper_data: dict) -> list:\n",
    "        \"\"\"修正后的章节处理方法\"\"\"\n",
    "        buffer = []\n",
    "        section_counter = [0]  # 使用可变对象保持计数状态\n",
    "\n",
    "        def _recursive_processor(sections: list, parent: dict = None, depth: int = 0) -> None:\n",
    "            for idx, section in enumerate(sections, start=1):\n",
    "                # 更新路径计数器\n",
    "                if depth >= len(section_counter):\n",
    "                    section_counter.append(0)\n",
    "                section_counter[depth] = idx\n",
    "                current_path = '.'.join(map(str, section_counter[:depth+1]))\n",
    "                \n",
    "                # 仅收集当前章节的独立文本（不含子章节）\n",
    "                buffer.append({\n",
    "                    \"section_path\": current_path,\n",
    "                    \"self_text\": section.get('text', ''),\n",
    "                    \"depth\": depth,\n",
    "                    \"title_chain\": _get_title_chain(section, parent),\n",
    "                    \"parent_path\": parent['section_path'] if parent else None,\n",
    "                    \"children\": []\n",
    "                })\n",
    "                \n",
    "                # 递归处理子章节\n",
    "                if section.get('subsections'):\n",
    "                    _recursive_processor(\n",
    "                        section['subsections'],\n",
    "                        parent=section,\n",
    "                        depth=depth + 1\n",
    "                    )\n",
    "                \n",
    "                # 重置深层计数器\n",
    "                for i in range(depth+1, len(section_counter)):\n",
    "                    section_counter[i] = 0\n",
    "\n",
    "        def _get_title_chain(section: dict, parent: dict) -> str:\n",
    "            \"\"\"生成标题层级链\"\"\"\n",
    "            chain = []\n",
    "            if parent and 'title_chain' in parent:\n",
    "                chain.append(parent['title_chain'])\n",
    "            chain.append(section.get('title', ''))\n",
    "            return ' > '.join(chain)\n",
    "\n",
    "        _recursive_processor(paper_data.get('sections', []))\n",
    "        return self._build_hierarchy(buffer)\n",
    "\n",
    "    def _build_hierarchy(self, flat_list: list) -> list:\n",
    "        \"\"\"将扁平列表转换为树形结构\"\"\"\n",
    "        node_map = {item['section_path']: item for item in flat_list}\n",
    "        for item in flat_list:\n",
    "            if item['parent_path']:\n",
    "                parent = node_map[item['parent_path']]\n",
    "                parent['children'].append(item)\n",
    "        return [item for item in flat_list if item['parent_path'] is None]\n",
    "\n",
    "    def generate_char_stats(self, buffer: list) -> dict:\n",
    "        \"\"\"修正后的统计方法\"\"\"\n",
    "        stats = {\n",
    "            \"individual_stats\": [],\n",
    "            \"cumulative_stats\": defaultdict(int),\n",
    "            \"summary\": defaultdict(int)\n",
    "        }\n",
    "\n",
    "        # 预计算独立字符数\n",
    "        for item in buffer:\n",
    "            self_chars = len(item['self_text'])\n",
    "            stats[\"individual_stats\"].append({\n",
    "                \"path\": item[\"section_path\"],\n",
    "                \"title_chain\": item[\"title_chain\"],\n",
    "                \"self_chars\": self_chars,\n",
    "                \"cumulative_chars\": self_chars + sum(len(c['self_text']) for c in item['children'])\n",
    "            })\n",
    "            stats[\"cumulative_stats\"][item[\"section_path\"]] = self_chars\n",
    "\n",
    "        # 后序遍历计算累计值\n",
    "        def _post_order(node):\n",
    "            total = len(node['self_text'])\n",
    "            for child in node['children']:\n",
    "                total += _post_order(child)\n",
    "            stats[\"cumulative_stats\"][node['section_path']] = total\n",
    "            return total\n",
    "\n",
    "        for root in [item for item in buffer if not item['parent_path']]:\n",
    "            _post_order(root)\n",
    "\n",
    "        # 生成摘要\n",
    "        total_self = sum(len(item['self_text']) for item in buffer)\n",
    "        stats[\"summary\"] = {\n",
    "            \"total_sections\": len(buffer),\n",
    "            \"total_self_chars\": total_self,\n",
    "            \"total_cumulative_chars\": stats[\"cumulative_stats\"].get('1', 0),\n",
    "            \"avg_self_per_section\": total_self // len(buffer) if buffer else 0,\n",
    "            \"max_depth\": max(item['depth'] for item in buffer),\n",
    "            \"longest_self_section\": max(buffer, key=lambda x: len(x['self_text']))['section_path'] if buffer else None,\n",
    "            \"longest_cumulative_section\": max(stats[\"cumulative_stats\"].items(), key=lambda x: x[1])[0]\n",
    "        }\n",
    "\n",
    "        return stats\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ProcessingConfig(\n",
    "    root_dir=r\"C:\\Users\\Inuyasha\\Programs\\Python\\AIGC\\Arxiv_Reviewer/research_papers\"\n",
    ")\n",
    "\n",
    "processor = PaperProcessor(config)\n",
    "result_path = processor.process(\n",
    "    url=\"https://arxiv.org/html/2501.00092v1\",\n",
    "    keyword=\"AI\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在paper_storage.py中增强保存函数\n",
    "def save_paper_data(data: dict, filename: str, encoding='utf-8'):\n",
    "    \"\"\"增强保存函数\"\"\"\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filename, 'w', encoding=encoding, errors='replace') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化配置和处理器（使用用户提供的路径）\n",
    "config = ProcessingConfig(\n",
    "    root_dir=r\"C:\\Users\\Inuyasha\\Programs\\Python\\AIGC\\Arxiv_Reviewer/research_papers\"\n",
    ")\n",
    "processor = PaperProcessor(config)\n",
    "\n",
    "# 加载已处理的论文数据\n",
    "paper_path = config.get_output_path(\"2501.00092\")  # 根据实际arxiv_id修改\n",
    "with open(paper_path, 'r', encoding='utf-8') as f:\n",
    "    paper_data = json.load(f)\n",
    "\n",
    "# 执行缓冲处理\n",
    "buffer = processor.process_sections_with_buffer(paper_data)\n",
    "\n",
    "# 查看处理结果\n",
    "for item in buffer[:2]:  # 查看前两项示例\n",
    "    print(f\"路径：{item['section_path']}\")\n",
    "    print(f\"标题链：{item['title_chain']}\")\n",
    "    print(f\"文本长度：{len(item['full_text'])}字符\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "# 生成统计报告\n",
    "stats = processor.generate_char_stats(buffer)\n",
    "\n",
    "# 控制台输出\n",
    "processor.print_stat_report(stats)\n",
    "\n",
    "# 获取原始数据（如需编程处理）\n",
    "print(\"\\n独立章节统计示例:\")\n",
    "for item in stats[\"individual_stats\"][:2]:\n",
    "    print(f\"路径 {item['path']} | 标题链: {item['title_chain'][:20]}... | 字符数: {item['chars']:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "分割后每个元素的长度:\n",
      "123\n",
      "7551\n",
      "7597\n",
      "6051\n",
      "2543\n",
      "7969\n",
      "7178\n",
      "7526\n",
      "7958\n",
      "3748\n",
      "7389\n",
      "7702\n",
      "7582\n",
      "814\n",
      "3009\n",
      "2936\n",
      "5114\n",
      "7539\n",
      "5148\n",
      "7758\n",
      "7469\n",
      "6276\n",
      "7334\n",
      "8005\n",
      "7528\n",
      "6816\n",
      "7539\n",
      "7191\n",
      "8010\n",
      "5851\n",
      "7661\n",
      "8074\n",
      "7341\n",
      "7780\n",
      "7647\n",
      "7930\n",
      "7250\n",
      "7372\n",
      "7899\n",
      "2645\n",
      "7162\n",
      "7862\n",
      "7976\n",
      "5336\n",
      "2009\n",
      "6391\n",
      "7821\n",
      "3739\n",
      "7823\n",
      "7410\n",
      "1589\n",
      "7703\n",
      "6813\n",
      "7250\n",
      "5595\n",
      "7851\n",
      "1166\n",
      "7718\n",
      "7665\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "# 加载已处理的论文数据\n",
    "paper_path = config.get_output_path(\"2501.00092\")  # 根据实际arxiv_id修改\n",
    "with open(paper_path, 'r', encoding='utf-8') as f:\n",
    "    paper_data = json.load(f)\n",
    "\n",
    "def extract_content_summary(paper_data: dict) -> List[str]:\n",
    "    \"\"\"完整字数统计与内容摘要\"\"\"\n",
    "    result = []\n",
    "    total_chars = 0\n",
    "    section_counter = [0]\n",
    "\n",
    "    # 处理Abstract\n",
    "    if 'abstract' in paper_data and paper_data['abstract']:\n",
    "        abstract = paper_data['abstract'].strip()\n",
    "        count = len(abstract)\n",
    "        preview = abstract[:100] + ('...' if len(abstract) > 100 else '')\n",
    "        result.append(f\"[Abstract] ({count}字)\\n{preview}\\n\")\n",
    "        total_chars += count\n",
    "\n",
    "    # 递归处理章节\n",
    "    def process_sections(sections: list, depth=0):\n",
    "        nonlocal total_chars\n",
    "        if depth >= len(section_counter):\n",
    "            section_counter.append(0)\n",
    "\n",
    "        for idx, section in enumerate(sections, start=1):\n",
    "            section_counter[depth] = idx\n",
    "            current_label = \".\".join(map(str, section_counter[:depth + 1]))\n",
    "            \n",
    "            if 'text' in section and section['text']:\n",
    "                text = section['text'].strip()\n",
    "                count = len(text)\n",
    "                preview = text\n",
    "                result.append(f\"[{current_label}] ({count}字)\\n{preview}\\n\")\n",
    "                total_chars += count\n",
    "\n",
    "            if 'subsections' in section and section['subsections']:\n",
    "                process_sections(section['subsections'], depth + 1)\n",
    "            \n",
    "            # 重置子级编号\n",
    "            for i in range(depth + 1, len(section_counter)):\n",
    "                section_counter[i] = 0\n",
    "\n",
    "    if 'sections' in paper_data and paper_data['sections']:\n",
    "        process_sections(paper_data['sections'])\n",
    "\n",
    "    # 添加总统计\n",
    "    result.append(\"=\" * 16 + f\"\\n总字数: {total_chars:,}字\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def split_long_paragraphs(content_list: List[str], max_length: int = 8000, overlap: int = 500) -> List[Dict]:\n",
    "    \"\"\"将长段落分割成更小的部分\"\"\"\n",
    "    def split_text(text: str) -> List[str]:\n",
    "        # 首先按换行符分割\n",
    "        lines = text.split('\\n')\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for line in lines:\n",
    "            if current_length + len(line) > max_length:\n",
    "                if current_chunk:\n",
    "                    chunks.append('\\n'.join(current_chunk))\n",
    "                    current_chunk = [current_chunk[-1]]\n",
    "                    current_length = len(current_chunk[-1])\n",
    "                \n",
    "                if len(line) > max_length:\n",
    "                    sentences = re.split('([。！？])', line)\n",
    "                    temp_chunk = []\n",
    "                    for i in range(0, len(sentences), 2):\n",
    "                        sentence = sentences[i] + (sentences[i+1] if i+1 < len(sentences) else '')\n",
    "                        if current_length + len(sentence) <= max_length:\n",
    "                            temp_chunk.append(sentence)\n",
    "                            current_length += len(sentence)\n",
    "                        else:\n",
    "                            if temp_chunk:\n",
    "                                chunks.append(''.join(temp_chunk))\n",
    "                            temp_chunk = [sentence]\n",
    "                            current_length = len(sentence)\n",
    "                    current_chunk.extend(temp_chunk)\n",
    "                else:\n",
    "                    current_chunk.append(line)\n",
    "                    current_length += len(line)\n",
    "            else:\n",
    "                current_chunk.append(line)\n",
    "                current_length += len(line)\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append('\\n'.join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    result = []\n",
    "    for item in content_list:\n",
    "        if len(item) <= max_length:\n",
    "            result.append({\"content\": item, \"word_count\": len(item)})\n",
    "        else:\n",
    "            split_parts = split_text(item)\n",
    "            for idx, part in enumerate(split_parts):\n",
    "                result.append({\n",
    "                    \"content\": part,\n",
    "                    \"word_count\": len(part),\n",
    "                    \"part\": idx + 1,\n",
    "                    \"total_parts\": len(split_parts)\n",
    "                })\n",
    "\n",
    "    return result\n",
    "# 获取内容摘要\n",
    "content_summary = extract_content_summary(paper_data)\n",
    "\n",
    "# 打印分割前每个元素的长度\n",
    "# print(\"分割前每个元素的长度:\")\n",
    "# for item in content_summary:\n",
    "#     print(len(item),item)\n",
    "\n",
    "# 分割长段落\n",
    "split_result = split_long_paragraphs(content_summary)\n",
    "print(len(split_result))\n",
    "# 打印分割后每个元素的长度\n",
    "print(\"分割后每个元素的长度:\")\n",
    "for item in split_result:\n",
    "    print(len(item['content']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
