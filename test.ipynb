{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from Arxiv_Parser.paper_parser import parse_html\n",
    "from Arxiv_Parser.paper_storage import save_paper_data\n",
    "from LLM.llm import MultiLLM\n",
    "from Task_Conductor.prompts import RelevanceTask\n",
    "\n",
    "class ProcessingConfig:\n",
    "    \"\"\"é…ç½®ä¸­å¿ƒï¼ˆç®€åŒ–ç‰ˆï¼‰\"\"\"\n",
    "    DEFAULT_SAVE_DIR = \"papers\"\n",
    "    FILENAME_TEMPLATE = \"paper_{arxiv_id}.json\"\n",
    "    \n",
    "    def __init__(self, root_dir: str = \".\", custom_output: Optional[str] = None):\n",
    "        self.root_dir = Path(root_dir).expanduser().resolve()\n",
    "        self.custom_output = Path(custom_output) if custom_output else None\n",
    "        self._init_paths()\n",
    "    \n",
    "    def _init_paths(self):\n",
    "        \"\"\"åˆå§‹åŒ–è·¯å¾„ï¼ˆå»é™¤HTMLç¼“å­˜ï¼‰\"\"\"\n",
    "        self.STATE_DIR = self.root_dir / \".status\"\n",
    "        self.OUTPUT_DIR = self.root_dir / self.DEFAULT_SAVE_DIR\n",
    "        \n",
    "        self.STATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get_output_path(self, arxiv_id: str) -> Path:\n",
    "        \"\"\"è§£ææœ€ç»ˆè¾“å‡ºè·¯å¾„\"\"\"\n",
    "        if self.custom_output:\n",
    "            if self.custom_output.is_dir():\n",
    "                return self.custom_output / self.FILENAME_TEMPLATE.format(arxiv_id=arxiv_id)\n",
    "            return self.custom_output\n",
    "        return self.OUTPUT_DIR / self.FILENAME_TEMPLATE.format(arxiv_id=arxiv_id)\n",
    "\n",
    "class PaperProcessor:\n",
    "    \"\"\"å¢å¼ºå‹è®ºæ–‡å¤„ç†å™¨ï¼ˆæ— HTMLç¼“å­˜ï¼‰\"\"\"\n",
    "    \n",
    "    VERSION = \"2.1\"\n",
    "    ENCODING = 'utf-8'\n",
    "    \n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.llm = MultiLLM('deepseek-coder')\n",
    "        self._state = {\n",
    "            \"current_step\": None,\n",
    "            \"metadata\": {},\n",
    "            \"stats\": {\n",
    "                \"sections\": 0,\n",
    "                \"references\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process(self, url: str, keyword: Optional[str] = None) -> Path:\n",
    "        \"\"\"æ ¸å¿ƒå¤„ç†æµç¨‹\"\"\"\n",
    "        try:\n",
    "            # åˆå§‹åŒ–å…ƒæ•°æ®\n",
    "            arxiv_id = self._extract_arxiv_id(url)\n",
    "            output_path = self.config.get_output_path(arxiv_id)\n",
    "            \n",
    "            self._update_state({\n",
    "                \"metadata\": {\n",
    "                    \"source_url\": url,\n",
    "                    \"arxiv_id\": arxiv_id,\n",
    "                    \"keyword\": keyword,\n",
    "                    \"output_path\": str(output_path),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            # è·å–å¹¶å¤„ç†å†…å®¹\n",
    "            print(\"ğŸ”„ è·å–è®ºæ–‡å†…å®¹...\", end='', flush=True)\n",
    "            response = requests.get(url)\n",
    "            response.encoding = self.ENCODING\n",
    "            response.raise_for_status()\n",
    "            print(\"âœ…\")\n",
    "            \n",
    "            # è§£æå†…å®¹\n",
    "            print(\"ğŸ” è§£æè®ºæ–‡ç»“æ„...\", end='', flush=True)\n",
    "            paper_data = parse_html(response.text)\n",
    "            self._update_state({\n",
    "                \"stats\": {\n",
    "                    \"sections\": len(paper_data.get(\"sections\", [])),\n",
    "                    \"references\": len(paper_data.get(\"references\", []))\n",
    "                }\n",
    "            })\n",
    "            print(f\"âœ… æ‰¾åˆ° {self._state['stats']['sections']} ä¸ªç« èŠ‚\")\n",
    "            \n",
    "            # ä¿å­˜ç»“æœ\n",
    "            print(f\"ğŸ’¾ ä¿å­˜åˆ°ï¼š{output_path}...\", end='', flush=True)\n",
    "            save_paper_data(\n",
    "                paper_data, \n",
    "                str(output_path),\n",
    "                encoding=self.ENCODING\n",
    "            )\n",
    "            print(f\"âœ… ({output_path.stat().st_size / 1024:.1f} KB)\")\n",
    "            \n",
    "            # å…³é”®è¯åˆ†æ\n",
    "            if keyword:\n",
    "                print(f\"ğŸ” åˆ†æå…³é”®è¯ '{keyword}' ç›¸å…³æ€§...\")\n",
    "                score = self._analyze_relevance(paper_data[\"abstract\"], keyword)\n",
    "                print(f\"â­ ç›¸å…³æ€§è¯„åˆ†ï¼š{score}/1.0\")\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"step\": self._state.get(\"current_step\"),\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"message\": str(e),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "            print(f\"\\nâŒ å¤„ç†å¤±è´¥ï¼š{error_info['message']}\")\n",
    "            raise RuntimeError(json.dumps(error_info, ensure_ascii=False)) from e\n",
    "    \n",
    "    def _analyze_relevance(self, abstract: str, keyword: str) -> float:\n",
    "        \"\"\"æ‰§è¡Œç›¸å…³æ€§åˆ†æ\"\"\"\n",
    "        task = RelevanceTask(abstract, keyword)\n",
    "        response = self.llm.ask(task.generate_prompt())\n",
    "        return task.parse_model_output(response)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_arxiv_id(url: str) -> str:\n",
    "        \"\"\"å¢å¼ºå‹IDæå–\"\"\"\n",
    "        base_id = url.split(\"/\")[-1]\n",
    "        for substr in [\"v1\", \"html/\", \"pdf/\"]:\n",
    "            base_id = base_id.replace(substr, \"\")\n",
    "        return base_id.strip(\"/\")\n",
    "    \n",
    "    def _update_state(self, update_dict: dict):\n",
    "        \"\"\"æ›´æ–°çŠ¶æ€\"\"\"\n",
    "        self._state.update(update_dict)\n",
    "\n",
    "# åœ¨paper_storage.pyä¸­å¢å¼ºä¿å­˜å‡½æ•°\n",
    "def save_paper_data(data: dict, filename: str, encoding='utf-8'):\n",
    "    \"\"\"å¢å¼ºä¿å­˜å‡½æ•°\"\"\"\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filename, 'w', encoding=encoding, errors='replace') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ è·å–è®ºæ–‡å†…å®¹..."
     ]
    }
   ],
   "source": [
    "config = ProcessingConfig(\n",
    "    root_dir=r\"C:\\Users\\Inuyasha\\Programs\\Python\\AIGC\\Arxiv_Reviewer/research_papers\"\n",
    ")\n",
    "\n",
    "processor = PaperProcessor(config)\n",
    "result_path = processor.process(\n",
    "    url=\"https://arxiv.org/html/2501.00092v1\",\n",
    "    keyword=\"AI\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
