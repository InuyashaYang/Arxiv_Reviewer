{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "from Arxiv_Parser.paper_parser import parse_html\n",
    "from Arxiv_Parser.paper_storage import save_paper_data\n",
    "from LLM.llm import MultiLLM\n",
    "from Task_Conductor.prompts import RelevanceTask\n",
    "\n",
    "class ProcessingConfig:\n",
    "    \"\"\"配置中心（简化版）\"\"\"\n",
    "    DEFAULT_SAVE_DIR = \"papers\"\n",
    "    FILENAME_TEMPLATE = \"paper_{arxiv_id}.json\"\n",
    "    \n",
    "    def __init__(self, root_dir: str = \".\", custom_output: Optional[str] = None):\n",
    "        self.root_dir = Path(root_dir).expanduser().resolve()\n",
    "        self.custom_output = Path(custom_output) if custom_output else None\n",
    "        self._init_paths()\n",
    "    \n",
    "    def _init_paths(self):\n",
    "        \"\"\"初始化路径（去除HTML缓存）\"\"\"\n",
    "        self.STATE_DIR = self.root_dir / \".status\"\n",
    "        self.OUTPUT_DIR = self.root_dir / self.DEFAULT_SAVE_DIR\n",
    "        \n",
    "        self.STATE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        self.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def get_output_path(self, arxiv_id: str) -> Path:\n",
    "        \"\"\"解析最终输出路径\"\"\"\n",
    "        if self.custom_output:\n",
    "            if self.custom_output.is_dir():\n",
    "                return self.custom_output / self.FILENAME_TEMPLATE.format(arxiv_id=arxiv_id)\n",
    "            return self.custom_output\n",
    "        return self.OUTPUT_DIR / self.FILENAME_TEMPLATE.format(arxiv_id=arxiv_id)\n",
    "\n",
    "class PaperProcessor:\n",
    "    \"\"\"增强型论文处理器（无HTML缓存）\"\"\"\n",
    "    \n",
    "    VERSION = \"2.1\"\n",
    "    ENCODING = 'utf-8'\n",
    "    \n",
    "    def __init__(self, config: ProcessingConfig):\n",
    "        self.config = config\n",
    "        self.llm = MultiLLM('deepseek-coder')\n",
    "        self._state = {\n",
    "            \"current_step\": None,\n",
    "            \"metadata\": {},\n",
    "            \"stats\": {\n",
    "                \"sections\": 0,\n",
    "                \"references\": 0\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def process(self, url: str, keyword: Optional[str] = None) -> Path:\n",
    "        \"\"\"核心处理流程\"\"\"\n",
    "        try:\n",
    "            # 初始化元数据\n",
    "            arxiv_id = self._extract_arxiv_id(url)\n",
    "            output_path = self.config.get_output_path(arxiv_id)\n",
    "            \n",
    "            self._update_state({\n",
    "                \"metadata\": {\n",
    "                    \"source_url\": url,\n",
    "                    \"arxiv_id\": arxiv_id,\n",
    "                    \"keyword\": keyword,\n",
    "                    \"output_path\": str(output_path),\n",
    "                    \"timestamp\": datetime.now().isoformat()\n",
    "                }\n",
    "            })\n",
    "            \n",
    "            # 获取并处理内容\n",
    "            print(\"🔄 获取论文内容...\", end='', flush=True)\n",
    "            response = requests.get(url)\n",
    "            response.encoding = self.ENCODING\n",
    "            response.raise_for_status()\n",
    "            print(\"✅\")\n",
    "            \n",
    "            # 解析内容\n",
    "            print(\"🔍 解析论文结构...\", end='', flush=True)\n",
    "            paper_data = parse_html(response.text)\n",
    "            self._update_state({\n",
    "                \"stats\": {\n",
    "                    \"sections\": len(paper_data.get(\"sections\", [])),\n",
    "                    \"references\": len(paper_data.get(\"references\", []))\n",
    "                }\n",
    "            })\n",
    "            print(f\"✅ 找到 {self._state['stats']['sections']} 个章节\")\n",
    "            \n",
    "            # 保存结果\n",
    "            print(f\"💾 保存到：{output_path}...\", end='', flush=True)\n",
    "            save_paper_data(\n",
    "                paper_data, \n",
    "                str(output_path),\n",
    "                encoding=self.ENCODING\n",
    "            )\n",
    "            print(f\"✅ ({output_path.stat().st_size / 1024:.1f} KB)\")\n",
    "            \n",
    "            # 关键词分析\n",
    "            if keyword:\n",
    "                print(f\"🔎 分析关键词 '{keyword}' 相关性...\")\n",
    "                score = self._analyze_relevance(paper_data[\"abstract\"], keyword)\n",
    "                print(f\"⭐ 相关性评分：{score}/1.0\")\n",
    "            \n",
    "            return output_path\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_info = {\n",
    "                \"step\": self._state.get(\"current_step\"),\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"message\": str(e),\n",
    "                \"traceback\": traceback.format_exc()\n",
    "            }\n",
    "            print(f\"\\n❌ 处理失败：{error_info['message']}\")\n",
    "            raise RuntimeError(json.dumps(error_info, ensure_ascii=False)) from e\n",
    "    \n",
    "    def _analyze_relevance(self, abstract: str, keyword: str) -> float:\n",
    "        \"\"\"执行相关性分析\"\"\"\n",
    "        task = RelevanceTask(abstract, keyword)\n",
    "        response = self.llm.ask(task.generate_prompt())\n",
    "        return task.parse_model_output(response)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _extract_arxiv_id(url: str) -> str:\n",
    "        \"\"\"增强型ID提取\"\"\"\n",
    "        base_id = url.split(\"/\")[-1]\n",
    "        for substr in [\"v1\", \"html/\", \"pdf/\"]:\n",
    "            base_id = base_id.replace(substr, \"\")\n",
    "        return base_id.strip(\"/\")\n",
    "    \n",
    "    def _update_state(self, update_dict: dict):\n",
    "        \"\"\"更新状态\"\"\"\n",
    "        self._state.update(update_dict)\n",
    "\n",
    "# 在paper_storage.py中增强保存函数\n",
    "def save_paper_data(data: dict, filename: str, encoding='utf-8'):\n",
    "    \"\"\"增强保存函数\"\"\"\n",
    "    Path(filename).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(filename, 'w', encoding=encoding, errors='replace') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 获取论文内容..."
     ]
    }
   ],
   "source": [
    "config = ProcessingConfig(\n",
    "    root_dir=r\"C:\\Users\\Inuyasha\\Programs\\Python\\AIGC\\Arxiv_Reviewer/research_papers\"\n",
    ")\n",
    "\n",
    "processor = PaperProcessor(config)\n",
    "result_path = processor.process(\n",
    "    url=\"https://arxiv.org/html/2501.00092v1\",\n",
    "    keyword=\"AI\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
